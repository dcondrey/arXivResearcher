{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv Research Intelligence Analysis\n",
    "\n",
    "This notebook provides interactive analysis of the collected arXiv dataset.\n",
    "\n",
    "**What you can explore:**\n",
    "- Research trends over time\n",
    "- Citation patterns and success factors\n",
    "- Research gaps and opportunities\n",
    "- Author networks and influence\n",
    "- Method and topic evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.rcParams['figure.figsize'] = (12, 6)\n",
    "    HAS_PLOTTING = True\n",
    "except ImportError:\n",
    "    print(\"Install matplotlib and seaborn for visualizations:\")\n",
    "    print(\"  uv add matplotlib seaborn\")\n",
    "    HAS_PLOTTING = False\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_DIR = \"../arxiv_research_data\"  # Adjust path as needed\n",
    "\n",
    "# Load papers\n",
    "with open(f\"{DATA_DIR}/papers.json\", \"r\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "print(f\"Loaded {len(df):,} papers\")\n",
    "print(f\"Date range: {df['published_date'].min()} to {df['published_date'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analysis results\n",
    "ANALYSIS_DIR = f\"{DATA_DIR}/analysis\"\n",
    "\n",
    "def load_analysis(name):\n",
    "    try:\n",
    "        with open(f\"{ANALYSIS_DIR}/{name}.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "text_analysis = load_analysis(\"text_analysis\")\n",
    "trend_analysis = load_analysis(\"trend_analysis\")\n",
    "gap_analysis = load_analysis(\"gap_analysis\")\n",
    "impact_analysis = load_analysis(\"impact_analysis\")\n",
    "network_analysis = load_analysis(\"network_analysis\")\n",
    "\n",
    "print(\"Analysis files loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nTotal papers: {len(df):,}\")\n",
    "print(f\"Unique categories: {df['primary_category'].nunique()}\")\n",
    "print(f\"Unique authors: {len(set(a for authors in df['authors'].dropna() for a in authors.split('; ')))}\")\n",
    "\n",
    "# Citation stats\n",
    "if 'citations' in df.columns:\n",
    "    citations = df['citations'].dropna()\n",
    "    print(f\"\\nCitation Statistics:\")\n",
    "    print(f\"  Total: {citations.sum():,.0f}\")\n",
    "    print(f\"  Mean: {citations.mean():.1f}\")\n",
    "    print(f\"  Median: {citations.median():.0f}\")\n",
    "    print(f\"  Max: {citations.max():.0f}\")\n",
    "\n",
    "# Coverage\n",
    "print(f\"\\nData Coverage:\")\n",
    "print(f\"  Has code: {df['has_code'].sum() if 'has_code' in df.columns else 'N/A'} ({100*df['has_code'].mean():.1f}%)\" if 'has_code' in df.columns else \"\")\n",
    "print(f\"  Has TL;DR: {df['tldr'].notna().sum()} ({100*df['tldr'].notna().mean():.1f}%)\" if 'tldr' in df.columns else \"\")\n",
    "print(f\"  Has venue: {df['publication_venue'].notna().sum()} ({100*df['publication_venue'].notna().mean():.1f}%)\" if 'publication_venue' in df.columns else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Papers by category\n",
    "if HAS_PLOTTING:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Top 15 categories by paper count\n",
    "    cat_counts = df['primary_category'].value_counts().head(15)\n",
    "    ax1 = axes[0]\n",
    "    cat_counts.plot(kind='barh', ax=ax1, color='steelblue')\n",
    "    ax1.set_xlabel('Number of Papers')\n",
    "    ax1.set_ylabel('Category')\n",
    "    ax1.set_title('Top 15 Categories by Paper Count')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Papers over time\n",
    "    ax2 = axes[1]\n",
    "    df['month'] = df['published_date'].str[:7]\n",
    "    monthly = df.groupby('month').size()\n",
    "    monthly.plot(ax=ax2, marker='o', color='steelblue')\n",
    "    ax2.set_xlabel('Month')\n",
    "    ax2.set_ylabel('Papers')\n",
    "    ax2.set_title('Papers Published Over Time')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emerging terms\n",
    "if text_analysis and 'emerging_terms' in text_analysis:\n",
    "    emerging = text_analysis['emerging_terms'].get('emerging_terms', {})\n",
    "    \n",
    "    print(\"EMERGING TERMS (growing in frequency)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    emerging_df = pd.DataFrame([\n",
    "        {'term': k, **v} for k, v in list(emerging.items())[:30]\n",
    "    ])\n",
    "    \n",
    "    if len(emerging_df) > 0:\n",
    "        display(emerging_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method trends\n",
    "if trend_analysis and 'method_trends' in trend_analysis:\n",
    "    methods = trend_analysis['method_trends']\n",
    "    \n",
    "    print(\"EMERGING METHODS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    emerging_methods = methods.get('emerging_methods', [])\n",
    "    if emerging_methods:\n",
    "        em_df = pd.DataFrame(emerging_methods)\n",
    "        display(em_df)\n",
    "    \n",
    "    print(\"\\nDECLINING METHODS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    declining_methods = methods.get('declining_methods', [])\n",
    "    if declining_methods:\n",
    "        dm_df = pd.DataFrame(declining_methods)\n",
    "        display(dm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category trends\n",
    "if trend_analysis and 'category_trends' in trend_analysis:\n",
    "    cat_trends = trend_analysis['category_trends']\n",
    "    \n",
    "    trends_df = pd.DataFrame([\n",
    "        {'category': k, **{kk: vv for kk, vv in v.items() if kk != 'time_series'}}\n",
    "        for k, v in cat_trends.items()\n",
    "    ]).sort_values('momentum', ascending=False)\n",
    "    \n",
    "    print(\"CATEGORY TRENDS BY MOMENTUM\")\n",
    "    print(\"=\" * 50)\n",
    "    display(trends_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot papers (high citation velocity)\n",
    "if trend_analysis and 'hot_papers' in trend_analysis:\n",
    "    hot = trend_analysis['hot_papers']\n",
    "    \n",
    "    print(\"ðŸ”¥ HOT PAPERS (Highest Citation Velocity)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, paper in enumerate(hot[:15], 1):\n",
    "        print(f\"\\n{i}. {paper['title'][:80]}{'...' if len(paper['title']) > 80 else ''}\")\n",
    "        print(f\"   Citations: {paper['citations']} | Velocity: {paper['citations_per_month']:.2f}/month\")\n",
    "        print(f\"   Category: {paper['category']} | Has Code: {paper.get('has_code', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Research Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top opportunities\n",
    "if gap_analysis and 'opportunity_scores' in gap_analysis:\n",
    "    opportunities = gap_analysis['opportunity_scores'].get('ranked_opportunities', [])\n",
    "    \n",
    "    print(\"ðŸŽ¯ TOP RESEARCH OPPORTUNITIES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    opp_df = pd.DataFrame(opportunities[:20])\n",
    "    if len(opp_df) > 0:\n",
    "        display(opp_df[['category', 'opportunity_score', 'paper_count', 'avg_citations', \n",
    "                        'avg_velocity', 'growth_indicator', 'recommendation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underexplored intersections\n",
    "if gap_analysis and 'category_intersections' in gap_analysis:\n",
    "    intersections = gap_analysis['category_intersections'].get('underexplored_pairs', [])\n",
    "    \n",
    "    print(\"ðŸ” UNDEREXPLORED CATEGORY INTERSECTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"These category combinations have fewer papers than expected:\")\n",
    "    print()\n",
    "    \n",
    "    for inter in intersections[:15]:\n",
    "        cats = ' + '.join(inter['categories'])\n",
    "        print(f\"  {cats}\")\n",
    "        print(f\"    Actual: {inter['actual_papers']} | Expected: {inter['expected_papers']} | Opportunity: {inter['opportunity_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey opportunities\n",
    "if gap_analysis and 'survey_opportunities' in gap_analysis:\n",
    "    surveys = gap_analysis['survey_opportunities'].get('category_survey_opportunities', [])\n",
    "    \n",
    "    print(\"ðŸ“š SURVEY PAPER OPPORTUNITIES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Categories with many papers but few/no surveys:\")\n",
    "    print()\n",
    "    \n",
    "    survey_df = pd.DataFrame(surveys[:15])\n",
    "    if len(survey_df) > 0:\n",
    "        display(survey_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility gaps\n",
    "if gap_analysis and 'reproducibility_gaps' in gap_analysis:\n",
    "    repro = gap_analysis['reproducibility_gaps']\n",
    "    \n",
    "    print(\"ðŸ”§ REPRODUCIBILITY OPPORTUNITIES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Highly-cited papers without code (implement for impact):\")\n",
    "    print()\n",
    "    \n",
    "    no_code = repro.get('high_cited_no_code', [])\n",
    "    for paper in no_code[:10]:\n",
    "        print(f\"  [{paper['citations']} citations] {paper['title'][:60]}...\")\n",
    "        print(f\"    Category: {paper['category']} | Methods: {', '.join(paper.get('methods', [])[:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Success Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What makes papers successful?\n",
    "if impact_analysis:\n",
    "    print(\"ðŸ† WHAT MAKES PAPERS SUCCESSFUL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Feature analysis\n",
    "    features = impact_analysis.get('feature_analysis', {})\n",
    "    \n",
    "    print(\"\\nBinary Features Impact:\")\n",
    "    for name, data in features.items():\n",
    "        if 'citation_lift' in data:\n",
    "            impact = data['impact']\n",
    "            lift = data['citation_lift']\n",
    "            symbol = 'âœ…' if impact == 'positive' else 'âŒ' if impact == 'negative' else 'âž–'\n",
    "            print(f\"  {symbol} {name}: {lift:.2f}x citations (with: {data['avg_citations_with']:.1f}, without: {data['avg_citations_without']:.1f})\")\n",
    "    \n",
    "    # Success formula\n",
    "    formula = impact_analysis.get('success_formula', {})\n",
    "    if formula:\n",
    "        print(f\"\\nðŸ“Š Success Formula:\")\n",
    "        print(f\"   {formula.get('interpretation', '')}\")\n",
    "        print(f\"\\n   Top factors: {', '.join(formula.get('top_factors', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citation distribution analysis\n",
    "if HAS_PLOTTING and 'citations' in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Citation distribution (log scale)\n",
    "    ax1 = axes[0]\n",
    "    citations = df['citations'].dropna()\n",
    "    citations_nonzero = citations[citations > 0]\n",
    "    ax1.hist(np.log10(citations_nonzero + 1), bins=50, color='steelblue', edgecolor='white')\n",
    "    ax1.set_xlabel('log10(Citations + 1)')\n",
    "    ax1.set_ylabel('Number of Papers')\n",
    "    ax1.set_title('Citation Distribution')\n",
    "    \n",
    "    # Citations by category\n",
    "    ax2 = axes[1]\n",
    "    top_cats = df['primary_category'].value_counts().head(10).index\n",
    "    cat_citations = df[df['primary_category'].isin(top_cats)].groupby('primary_category')['citations'].mean().sort_values(ascending=True)\n",
    "    cat_citations.plot(kind='barh', ax=ax2, color='steelblue')\n",
    "    ax2.set_xlabel('Average Citations')\n",
    "    ax2.set_ylabel('Category')\n",
    "    ax2.set_title('Average Citations by Category')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code availability impact\n",
    "if 'has_code' in df.columns and 'citations' in df.columns:\n",
    "    with_code = df[df['has_code'] == True]['citations'].dropna()\n",
    "    without_code = df[df['has_code'] == False]['citations'].dropna()\n",
    "    \n",
    "    print(\"CODE AVAILABILITY IMPACT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Papers WITH code:    {len(with_code):,} papers, avg citations: {with_code.mean():.1f}\")\n",
    "    print(f\"Papers WITHOUT code: {len(without_code):,} papers, avg citations: {without_code.mean():.1f}\")\n",
    "    print(f\"\\nCitation multiplier: {with_code.mean() / without_code.mean():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top authors\n",
    "if network_analysis and 'author_metrics' in network_analysis:\n",
    "    author_metrics = network_analysis['author_metrics']\n",
    "    \n",
    "    print(\"ðŸ‘¥ MOST INFLUENTIAL AUTHORS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # By citations\n",
    "    print(\"\\nTop by Total Citations:\")\n",
    "    for a in author_metrics.get('top_productive', [])[:10]:\n",
    "        print(f\"  {a['name'][:40]}: {a['paper_count']} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team size analysis\n",
    "if 'author_count' in df.columns:\n",
    "    print(\"TEAM SIZE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    team_stats = df.groupby(pd.cut(df['author_count'], bins=[0, 1, 3, 6, 100], \n",
    "                                    labels=['Solo', 'Small (2-3)', 'Medium (4-6)', 'Large (7+)'])).agg({\n",
    "        'arxiv_id': 'count',\n",
    "        'citations': 'mean'\n",
    "    }).rename(columns={'arxiv_id': 'papers', 'citations': 'avg_citations'})\n",
    "    \n",
    "    display(team_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Method & Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common methods\n",
    "if text_analysis and 'method_frequency' in text_analysis:\n",
    "    methods = text_analysis['method_frequency']\n",
    "    \n",
    "    print(\"ðŸ”§ MOST USED METHODS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    method_df = pd.DataFrame(list(methods.items()), columns=['method', 'count']).head(25)\n",
    "    display(method_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most mentioned datasets\n",
    "if text_analysis and 'dataset_frequency' in text_analysis:\n",
    "    datasets = text_analysis['dataset_frequency']\n",
    "    \n",
    "    print(\"ðŸ“Š MOST USED DATASETS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    dataset_df = pd.DataFrame(list(datasets.items()), columns=['dataset', 'count']).head(25)\n",
    "    display(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Analysis\n",
    "\n",
    "Use this section for your own explorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find papers about a specific topic\n",
    "def search_papers(query, n=20):\n",
    "    \"\"\"Search papers by title/abstract.\"\"\"\n",
    "    query = query.lower()\n",
    "    matches = df[\n",
    "        df['title'].str.lower().str.contains(query, na=False) |\n",
    "        df['abstract'].str.lower().str.contains(query, na=False)\n",
    "    ].nlargest(n, 'citations' if 'citations' in df.columns else 'arxiv_id')\n",
    "    \n",
    "    return matches[['title', 'citations', 'primary_category', 'published_date', 'arxiv_url']]\n",
    "\n",
    "# Example: search_papers(\"graph neural\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find highest-cited papers in a category\n",
    "def top_papers_in_category(category, n=15):\n",
    "    \"\"\"Get top-cited papers in a category.\"\"\"\n",
    "    cat_papers = df[df['primary_category'] == category].nlargest(n, 'citations')\n",
    "    return cat_papers[['title', 'citations', 'citations_per_month', 'has_code', 'arxiv_url']]\n",
    "\n",
    "# Example: top_papers_in_category('cs.LG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare two categories\n",
    "def compare_categories(cat1, cat2):\n",
    "    \"\"\"Compare two categories.\"\"\"\n",
    "    data = []\n",
    "    for cat in [cat1, cat2]:\n",
    "        subset = df[df['primary_category'] == cat]\n",
    "        data.append({\n",
    "            'category': cat,\n",
    "            'papers': len(subset),\n",
    "            'avg_citations': subset['citations'].mean() if 'citations' in subset else None,\n",
    "            'code_rate': subset['has_code'].mean() * 100 if 'has_code' in subset else None,\n",
    "            'avg_authors': subset['author_count'].mean() if 'author_count' in subset else None,\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Example: compare_categories('cs.LG', 'cs.CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exploration here!\n",
    "# Try:\n",
    "#   search_papers(\"your topic\")\n",
    "#   top_papers_in_category(\"cs.AI\")\n",
    "#   compare_categories(\"cs.LG\", \"stat.ML\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
